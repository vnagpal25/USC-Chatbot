{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Collecting deeppavlov\n",
      "  Using cached deeppavlov-1.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fastapi<=0.89.1,>=0.47.0 (from deeppavlov)\n",
      "  Using cached fastapi-0.89.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting filelock<3.10.0,>=3.0.0 (from deeppavlov)\n",
      "  Using cached filelock-3.9.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: nltk<3.10.0,>=3.2.4 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from deeppavlov) (3.8.1)\n",
      "Collecting numpy<1.24 (from deeppavlov)\n",
      "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 77, in _build_backend\n",
      "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
      "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/arshia/.miniforge3/envs/hackthon/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-78z0a4hl/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
      "  \u001b[31m   \u001b[0m     import setuptools.version\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-78z0a4hl/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     import pkg_resources\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-78z0a4hl/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
      "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Requirement already satisfied: beautifulsoup4 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: PyPDF2 in /home/arshia/.miniforge3/envs/hackthon/lib/python3.12/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Hackthon 2024, AE \n",
    "!pip install requests\n",
    "!pip install deeppavlov\n",
    "!pip install beautifulsoup4\n",
    "!pip install PyPDF2\n",
    "import requests \n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "# Finds the text inside of pdf \n",
    "def find_pdf_text(url):\n",
    "  resp = requests.get(url)\n",
    "  temp_file = \"temp_pdf.pdf\"\n",
    "  with open(temp_file, \"wb\") as f:\n",
    "    f.write(resp.content)\n",
    "\n",
    "  pdf_reader = PyPDF2.PdfReader(temp_file)\n",
    "  all_txt = \"\"\n",
    "  for page in pdf_reader.pages:\n",
    "    all_txt+= page.extract_text()\n",
    "  return all_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sc.edu/about/offices_and_divisions/dining_services/restaurants/index.php\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 421 https://www.sc.edu/study/colleges_schools/medicine_greenville/index.php\n",
      "3 431 https://www.sc.edu/study/colleges_schools/music/index.php\n",
      "4 438 https://www.sc.edu/study/colleges_schools/nursing/index.php\n",
      "5 441 https://www.sc.edu/study/colleges_schools/pharmacy/index.php\n",
      "6 462 https://www.sc.edu/study/colleges_schools/public_health/index.php\n",
      "7 465 https://www.sc.edu/study/colleges_schools/socialwork/index.php\n",
      "8 467 https://www.sc.edu/study/colleges_schools/honors_college/index.php\n",
      "9 484 https://www.sc.edu/about/employment/index.php\n",
      "261\n",
      "Queue file is written!\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "QUEUE_FILE = \"queue.csv\"\n",
    "\n",
    "URL = \"https://sc.edu/\"\n",
    "\n",
    "# The https prefix to only include website links\n",
    "PREFIX =\"https\"\n",
    "\n",
    "# The template that we will be using for usc related websites search\n",
    "TEMP = \"sc.edu\"\n",
    "text_list = []\n",
    "checked_links = set()\n",
    "added_text = set()\n",
    "\n",
    "\n",
    "num_to_run = int(input(\"Give the number of links you want to complete(int)\"))\n",
    "\n",
    "\n",
    "# Saving the queue of links that haven't been searched\n",
    "def save_queue(q_links):\n",
    "  with open(\"queue.csv\", \"w\") as f:\n",
    "    f.write(\", \".join(q_links))\n",
    "    print(\"Queue file is written!\")\n",
    "\n",
    "def save_text(text_arr):\n",
    "  clean_txt = []\n",
    "  for item in text_arr:\n",
    "    clean_txt.append(item)\n",
    "  print(len(clean_txt))\n",
    "  for i in range(len(clean_txt)):\n",
    "    # clean_text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    clean_txt[i] = re.sub(r'[^a-zA-Z@0-9?!_\\-.,;:\\' ]', '', clean_txt[i])\n",
    "    clean_txt[i]= re.sub(r'\\s+', ' ', clean_txt[i])\n",
    "  with open(\"search_output.json\", \"a\") as f:\n",
    "\n",
    "    json.dump(clean_txt, f)\n",
    "    # f.write(\", \".join(text_arr))\n",
    "\n",
    "# Returns page_txt, page_links\n",
    "def find_page_info(pag_url):\n",
    "  if (pag_url.endswith(\".pdf\")):\n",
    "      try:\n",
    "        return [], find_pdf_text(pag_url)\n",
    "      except:\n",
    "        return [], []\n",
    "\n",
    "  try:\n",
    "    resp = requests.get(pag_url)\n",
    "    # print(\"HERE\", resp.text)\n",
    "  except:\n",
    "    return [], []\n",
    "  try:\n",
    "    txt = resp.text\n",
    "    soup = bs4.BeautifulSoup(txt,  \"html.parser\")\n",
    "    links = soup.find_all('a')\n",
    "    clean_links = []\n",
    "  except:\n",
    "    return [], []\n",
    "  # Finding all the links in the page\n",
    "  for link in links:\n",
    "    clean_links.append(link.get('href'))\n",
    "  page_txt = []\n",
    "  ps = soup.find_all('p')\n",
    "  for p in ps:\n",
    "    page_txt.append(p.get_text())\n",
    "  return clean_links, page_txt\n",
    "\n",
    "def get_queue():\n",
    "  with open(\"queue.csv\", \"r\") as file:\n",
    "    content = file.read()\n",
    "    splited = content.split(\", \")\n",
    "  return splited\n",
    "\n",
    "\n",
    "queue_links = get_queue()\n",
    "if (not queue_links):\n",
    "  queue_links = [URL]\n",
    "\n",
    "\n",
    "\n",
    "counter = 1\n",
    "while(queue_links):\n",
    "  # getting the link in front of the queue \n",
    "  link_to_check = queue_links.pop(0)\n",
    "  checked_links.add(link_to_check)\n",
    "  links, page_txt = find_page_info(link_to_check)\n",
    "  for item in page_txt:\n",
    "    text_list.append(item)\n",
    "  for link in links:\n",
    "    if (link != None and (not link in checked_links) and (not link in queue_links) and (TEMP in link) and link.startswith(PREFIX)):\n",
    "      queue_links.append(link)\n",
    "  \n",
    "  for item in page_txt:\n",
    "    if (not item in added_text):\n",
    "      added_text.add(item)\n",
    "      text_list.append(item)\n",
    "  counter += 1\n",
    "\n",
    "  if (counter % 100 ==0):\n",
    "    save_text(text_list)\n",
    "    save_queue(queue_links)\n",
    "    queue_links = get_queue()\n",
    "    text_list = []\n",
    "    print(f\"Saved up to {counter}th iteration!\")\n",
    "\n",
    "  if (counter == num_to_run):\n",
    "    save_text(text_list)\n",
    "    save_queue(queue_links)\n",
    "    break\n",
    "  print(counter, len(queue_links), link_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58159\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "import numpy as np\n",
    "print(len(text_list))\n",
    "cleaned_tlist = []\n",
    "\n",
    "for i in range(len(text_list) // 100):\n",
    "  arr = np.array(text_list[i * 100: (i+1) *100])\n",
    "  arr = np.unique(arr)\n",
    "  for item in arr:\n",
    "    cleaned_tlist.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37659\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "print(len(cleaned_tlist))\n",
    "for i in range(len(cleaned_tlist)):\n",
    "  # clean_text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "  cleaned_tlist[i] = re.sub(r'[^a-zA-Z@0-9?!_\\-.,;:\\' ]', '', cleaned_tlist[i])\n",
    "  cleaned_tlist[i]= re.sub(r'\\s+', ' ', cleaned_tlist[i])\n",
    "\n",
    "with open (\"output.json\", \"w\") as f:\n",
    "  json.dump(cleaned_tlist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36904\n",
      "Were committed to providing the resources and support our students need to be successful in their academic pursuits. Get the most from every facet of your academic experience at South Carolina or connect with university resources that can help you reach your aspirations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"output.json\", \"r\") as f:\n",
    "  list = json.load(f)\n",
    "\n",
    "# print(len(cleaned_tlist))\n",
    "print(len(list))\n",
    "print(list[47])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackthon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
